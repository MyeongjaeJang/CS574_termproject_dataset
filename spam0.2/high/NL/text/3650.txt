
Statistical theory for deep neural networks 
Speaker Johannes Schmidt-Hieber (MI) Johannes is an assistant professor in the Statistical Science group at MI. He works on nonparametric and high-dimensional statistical problems. 
Abstract Deep neural networks are now a central topic in computer science. Although successful in many application it is unclear why and how they work. One route towards a mathematical theory is to think of deep networks as a statistical procedure. This allows us to analyse the performance in terms of bounds on the statistical risk and to compare deep networks to other existing methods. The talk gives a brief overview and explains recent results. Second Lecture 
An AlphaGo Inspired Solution for Chemical Retrosynthesis 
Speaker Mike Preuss (Westfälische Wilhelms-Universität, Münster, Germany). Mike Preuss is a research associate at the Department of Information Systems and Statistics at WWU. His research areas include Artificial Intelligence, Optimization, Games and Social Media Computing. He is visiting LIACS this week. 
Abstract To plan the syntheses of small organic molecules, chemists use retrosynthesis, a technique in which target molecules are recursively transformed into increasingly simpler precursors. Computer-aided retrosynthesis would be a valuable tool but at present it is slow and provides results of unsatisfactory quality. Here we use Monte Carlo tree search to discover retrosynthetic routes. We combined Monte Carlo tree search with an expansion policy network that guides the search, and a filter network to pre-select the most promising retrosynthetic steps. These deep neural networks were trained on essentially all reactions ever published in organic chemistry. Our system solves for almost twice as many molecules, thirty times faster than the traditional search method based on extracted rules and hand-designed heuristics. 